trained_on	eval_on	accuracy	f1	model	dataset	subset	timestamp	gen_params
relevant_context	relevant_context	0.6785714285714286	0.3315497469384024	llama	MenatQA	counterfactual	2024-11-05 12:15:32	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	relevant_context	0.6607142857142857	0.35697952986555925	llama	MenatQA	counterfactual	2024-11-05 12:15:36	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	wrong_date_context	0.6160714285714286	0.28988156742883636	llama	MenatQA	counterfactual	2024-11-05 12:15:41	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	wrong_date_context	0.6875	0.3671529552963376	llama	MenatQA	counterfactual	2024-11-05 12:15:46	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	random_context	0.6428571428571429	0.29938307345082554	llama	MenatQA	counterfactual	2024-11-05 12:15:51	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	random_context	0.6696428571428571	0.36287872912016	llama	MenatQA	counterfactual	2024-11-05 12:15:56	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	no_context	0.5625	0.28834314703675606	llama	MenatQA	counterfactual	2024-11-05 12:16:01	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	no_context	0.5714285714285714	0.3224844748658881	llama	MenatQA	counterfactual	2024-11-05 12:16:06	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	relevant_context	0.554945054945055	0.25225218830584045	llama	MenatQA	order	2024-11-05 12:16:11	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	relevant_context	0.5274725274725275	0.27761967413542615	llama	MenatQA	order	2024-11-05 12:16:16	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	wrong_date_context	0.5054945054945055	0.2337921989667303	llama	MenatQA	order	2024-11-05 12:16:21	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	wrong_date_context	0.521978021978022	0.2755140562590465	llama	MenatQA	order	2024-11-05 12:16:26	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	random_context	0.46153846153846156	0.18804222244752306	llama	MenatQA	order	2024-11-05 12:16:31	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	random_context	0.4945054945054945	0.222947640736315	llama	MenatQA	order	2024-11-05 12:16:36	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	no_context	0.4230769230769231	0.19253903167734326	llama	MenatQA	order	2024-11-05 12:16:40	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	no_context	0.4340659340659341	0.21712319068790006	llama	MenatQA	order	2024-11-05 12:16:45	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	relevant_context	0.3392857142857143	0.17778944049495743	llama	MenatQA	scope	2024-11-05 12:16:50	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	relevant_context	0.32142857142857145	0.19679813877858945	llama	MenatQA	scope	2024-11-05 12:16:55	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	wrong_date_context	0.29464285714285715	0.14693621038158852	llama	MenatQA	scope	2024-11-05 12:17:00	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	wrong_date_context	0.2857142857142857	0.17977677020785113	llama	MenatQA	scope	2024-11-05 12:17:05	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	random_context	0.16071428571428573	0.07879669324576952	llama	MenatQA	scope	2024-11-05 12:17:10	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	random_context	0.16071428571428573	0.0666718671887845	llama	MenatQA	scope	2024-11-05 12:17:15	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	no_context	0.16071428571428573	0.07094964524364634	llama	MenatQA	scope	2024-11-05 12:17:20	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	no_context	0.16071428571428573	0.09671309057393149	llama	MenatQA	scope	2024-11-05 12:17:25	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	relevant_context	0.4034090909090909	0.18784394174601401	llama	MenatQA	scope_expand	2024-11-05 12:17:30	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	relevant_context	0.32954545454545453	0.1931969765758993	llama	MenatQA	scope_expand	2024-11-05 12:17:35	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	wrong_date_context	0.32386363636363635	0.15165322645269166	llama	MenatQA	scope_expand	2024-11-05 12:17:40	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	wrong_date_context	0.32954545454545453	0.15905134465286094	llama	MenatQA	scope_expand	2024-11-05 12:17:45	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	random_context	0.29545454545454547	0.09569943735041885	llama	MenatQA	scope_expand	2024-11-05 12:17:50	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	random_context	0.2556818181818182	0.08306010334225923	llama	MenatQA	scope_expand	2024-11-05 12:17:54	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context	no_context	0.26704545454545453	0.0838703067338123	llama	MenatQA	scope_expand	2024-11-05 12:17:59	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context	no_context	0.25	0.08578212952377386	llama	MenatQA	scope_expand	2024-11-05 12:18:04	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	relevant_context	0.8660714285714286	0.43340460018506227	llama	MenatQA	counterfactual	2024-11-06 12:04:17	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	wrong_date_context	0.8571428571428571	0.42808091622429834	llama	MenatQA	counterfactual	2024-11-06 12:04:22	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	random_context	0.8660714285714286	0.44460822207670914	llama	MenatQA	counterfactual	2024-11-06 12:04:27	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	no_context	0.875	0.42996959564711645	llama	MenatQA	counterfactual	2024-11-06 12:04:32	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	relevant_context	0.6208791208791209	0.32105022274663014	llama	MenatQA	order	2024-11-06 12:04:37	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	wrong_date_context	0.6208791208791209	0.31723783178041	llama	MenatQA	order	2024-11-06 12:04:41	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	random_context	0.5824175824175825	0.2968386736082274	llama	MenatQA	order	2024-11-06 12:04:46	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	no_context	0.5879120879120879	0.28258067437123674	llama	MenatQA	order	2024-11-06 12:04:51	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	relevant_context	0.30357142857142855	0.17779419889658718	llama	MenatQA	scope	2024-11-06 12:04:56	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	wrong_date_context	0.2857142857142857	0.1546924319660388	llama	MenatQA	scope	2024-11-06 12:05:01	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	random_context	0.13392857142857142	0.09917721451542216	llama	MenatQA	scope	2024-11-06 12:05:06	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	no_context	0.16071428571428573	0.09435372633902044	llama	MenatQA	scope	2024-11-06 12:05:11	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	relevant_context	0.3181818181818182	0.17382401905955935	llama	MenatQA	scope_expand	2024-11-06 12:05:16	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	wrong_date_context	0.30113636363636365	0.15792550042875472	llama	MenatQA	scope_expand	2024-11-06 12:05:21	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	random_context	0.19318181818181818	0.09361596514002928	llama	MenatQA	scope_expand	2024-11-06 12:05:26	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
mixed_context-stacked	no_context	0.22727272727272727	0.10437208457763263	llama	MenatQA	scope_expand	2024-11-06 12:05:31	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	relevant_context	0.7857142857142857	0.4246483948136976	llama	MenatQA	counterfactual	2024-11-07 19:59:28	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	wrong_date_context	0.7857142857142857	0.4246483948136976	llama	MenatQA	counterfactual	2024-11-07 19:59:33	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	random_context	0.7857142857142857	0.4246483948136976	llama	MenatQA	counterfactual	2024-11-07 19:59:38	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	no_context	0.7857142857142857	0.4246483948136976	llama	MenatQA	counterfactual	2024-11-07 19:59:42	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	relevant_context	0.5164835164835165	0.2727475835988066	llama	MenatQA	order	2024-11-07 19:59:47	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	wrong_date_context	0.5164835164835165	0.2727475835988066	llama	MenatQA	order	2024-11-07 19:59:52	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	random_context	0.5164835164835165	0.2727475835988066	llama	MenatQA	order	2024-11-07 19:59:57	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	no_context	0.5164835164835165	0.2727475835988066	llama	MenatQA	order	2024-11-07 20:00:02	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	relevant_context	0.19642857142857142	0.1141892785669046	llama	MenatQA	scope	2024-11-07 20:00:07	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	wrong_date_context	0.19642857142857142	0.1141892785669046	llama	MenatQA	scope	2024-11-07 20:00:12	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	random_context	0.19642857142857142	0.1141892785669046	llama	MenatQA	scope	2024-11-07 20:00:17	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	no_context	0.19642857142857142	0.1141892785669046	llama	MenatQA	scope	2024-11-07 20:00:22	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	relevant_context	0.17045454545454544	0.09888016019498223	llama	MenatQA	scope_expand	2024-11-07 20:00:27	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	wrong_date_context	0.17045454545454544	0.09888016019498223	llama	MenatQA	scope_expand	2024-11-07 20:00:32	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	random_context	0.17045454545454544	0.09888016019498223	llama	MenatQA	scope_expand	2024-11-07 20:00:37	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}
relevant_context-24epochs	no_context	0.17045454545454544	0.09888016019498223	llama	MenatQA	scope_expand	2024-11-07 20:00:42	{'max_new_tokens': 16, 'do_sample': True, 'temperature': 0.1, 'top_p': 0.1}