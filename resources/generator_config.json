{
  "MenatQA_gemma_relevant": {
    "max_new_tokens": 128,
    "do_sample": true,
    "temperature": 0.1,
    "top_p": 0.9,
    "repetition_penalty": 1.6,
    "penalty_alpha": 0.1
  },
  "mistral": {
    "max_new_tokens": 16,
    "do_sample": true,
    "temperature": 1.9,
    "top_p": 0.1
  },
  "llama": {
    "max_new_tokens": 16,
    "do_sample": true,
    "temperature": 1.9,
    "top_p": 0.1
  }
}
